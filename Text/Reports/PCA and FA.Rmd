---
title: "PCA exploration"
author: "Tom Tribe"
date: "2022-09-21"
classoption: 12pt
output: bookdown::pdf_document2
bibliography: ./MASTER.bib
header-includes: \usepackage{float}
                    \floatplacement{figure}{H}
                    \floatplacement{table}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(ggbiplot)
library(factoextra)

```

# Load the data

```{r}
diamonds <- read.csv("./diamonds.csv", encoding = "UTF-8")
diamonds[1:5,]
diamonds$X <- NULL
diamonds$cut <- factor(diamonds$cut, 
                       levels = c("Fair","Good","Very Good","Premium","Ideal"))
diamonds$color <- factor(diamonds$color, 
                         levels = c("J","I","H","G","F","E","D"))
diamonds$clarity <- factor(diamonds$clarity,
                           levels = c("I1","SI2","SI1","VS2","VS1","VVS2","VVS1","IF"))
diamonds_num <- subset(diamonds, select = c(carat,depth,table,price,x,y,z))
```

## Initial PCA

```{r}
PCA.diamonds <- prcomp(diamonds[,-c(2:4)], center = TRUE, scale = TRUE)
plot(PCA.diamonds)
summary(PCA.diamonds)
```

We see that the first principal component explains the majority of the variance (68%) in the dataset with second (18%) and third (10%) principal components explaining a noticeable amount. Components beyond the third principal component explain minimal variance (less than 4%). This indicates we can describe the data using only the first three principal components without losing much information.

```{r}
PCA.diamonds$rotation
```

Looking at the eigenvectors we see that PC1 is primarily defined by carat, price, x, y and z. PC1 seems to be defined by dimension variables and, as we saw in the correlation matrix, price is closely associated with these variables. The variables with the most weight in PC2 are depth and table. We see a similar pattern in PC3 with depth a table having the largest weighting. This makes sense as in our examination of correlations between variables we saw that price, x, y, z and carat were all strongly correlated. The opposite loadings of depth and table in PC2 and PC3 reflect there negative correlation.  A visualization of the PCA is shown in the Biplot below. 


```{r}
ggbiplot(PCA.diamonds, obs.scale =1, var.scale = 1)
```

Due to the vast number of data entries it is hard to interpret what is going on from the visual display. We will not take a smaller sample of 1000 from the diamonds data and redo the PCA after first checking that the smaller sample has similar properties to the full sample.

```{r}
set.seed(300525287, kind = "Mersenne-Twister")
smallersample <- diamonds[sample(nrow(diamonds), "500"), ]
PCA <- prcomp(smallersample[,-c(2:4)], center = TRUE, scale = TRUE)
```

```{r}
plot(PCA)
summary(PCA)
PCA$rotation
```

```{r}
par(mfrow=c(1,2))
plot(PCA)
screeplot(PCA, type = "line", main = "Screeplot")
```

We see that a PCA on the smaller sample results in close to identical results. The first PC now accounts  for 69% of the variance, the second PC accounts for 20% and the third PC accounts for 9%. Together the first three components account for 97% of the variance in the dataset.

```{r}
ggbiplot(reducedpca, obs.scale =1, var.scale = 1, varname.size = 5)
```

We see that carat, x, y, z and price are strongly influence the first PC. While depth and table strongly (and reasonably equally) influence the second PC. We see evidence of the negative correlation between table and depth and a close to zero correlation between these two variables and the variables that strongly influence PC1. We also have an indication that there is redundancy between carat, x, y, z and price. 


In this investigation we are interested in predicting the price of diamonds. To do this we will perform a regression with principal components with price as the dependent variable and the other numerical variables as explanatory variables. 

## Principal Components Regression

We will begin our principal components regression with a multiple linear regression with all the numeric variables. We again use the smaller sample to make interpretation of visual displays easier.

```{r}
diamondscale <- scale(smallersample[,-c(2:4)])
diamondscale <- as.data.frame(diamondscale)
diamondsmod <- lm(price ~ carat + x + y + z + depth + table, data = diamondscale)
summary(diamondsmod)
```

We see from this output that all but one predictor variable (y) are significant in this model when the effects of the other predictors are already accounted for. This model explain 86% of the variation in price. We will now check the regression assumptions.

```{r}
plot(diamondsmod)
```

The regression diagnostic plots indicate significant deviations from the assumptions of a linear regression. It appears the assumptions of linearity, normality and homogeneity of variance have all been violated as well as several highly influential points. 


We will now perform a principal regression analysis with price as the response variable and and all other numerical variables as the explanatory variables. 

```{r}
PCAprice <- prcomp(smallersample[,-c(2:4,7)], center = TRUE, scale = TRUE)
plot(PCAprice)
summary(PCAprice)
```

The plot above shows the proportion of variation explained by each of the principal component. The first three principal components explain nearly all (98%) the variation in the data. 

```{r}
PCAprice$rotation
```

We see that in the first PC carat, x, y and z all have equal weighting. In the second PC depth and table have the strongest weighting. 

```{r}
ggbiplot(PCAprice, obs.scale =1, var.scale = 1, groups = smallersample$clarity, ellipse = TRUE)
```


Even with price not included we still see the same general structure as the PCA with price.  

```{r}
PCApricevars <- prcomp(smallersample[,-c(2:4,7)], center = TRUE, scale = TRUE)$x
diamondpricePCA <- lm(smallersample$price ~ PCApricevars[,1] + PCApricevars[,2] + PCApricevars[,3] + PCApricevars[,4] + PCApricevars[,5] + PCApricevars[,6])
summary(diamondpricePCA)
step(diamondpricePCA, direction = "both",  k=log(53940))
diamondpricePCA2 <- lm(smallersample$price ~ PCApricevars[,1] + PCApricevars[,2])
summary(diamondpricePCA2)
```

We see that every single principal component is found to be significant in this model when the effects of the other principal components is taken into account. Therefore we have failed to create a more parsimonious model for predicting price through using regression with principal components. It is possible that PCR does not work well with diamonds dataset as the categorical variables of cut, clarity and color are also important predictors of price but these could not be included in the PCR model.


```{r}
Rotation.df <- data.frame(t(reducedpca$rotation))
Rotation.df$PC <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7")
Rotation.melt <- melt(Rotation.df, id.vars = "PC")
ggplot(Rotation.melt[c(1,8, 15,22,29,36,43),]) +
  geom_line(aes(x = variable, y = value, group = PC, col = PC), size = 3)
```

```{r}
Rotation.df <- data.frame(t(reducedpca$rotation))
Rotation.df$PC <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7")
Rotation.melt <- melt(Rotation.df, id.vars = "PC")
ggplot(Rotation.melt[c(2,9, 16,23,30,37,44),]) +
  geom_line(aes(x = variable, y = value, group = PC, col = PC), size = 3)
```

## Factor Analysis

In our investigation of the principal components of the dataset we observed that the first two principal components explained the the vast majority of the variation in the data.  The first was the strong positive correlation between price and the dimension variables (x, y, z, carat) which we may call price + dimension. The second was the negative correlation between depth and table which we might combine into light performance as both variables are crucial in giving a diamond its "sparkle". We hypothesise that these two factors may explain the observations in the diamonds dataset. To investigate this we will conduct a factor analysis. 

```{r}
factanal(diamonds_num, factors =2)
```

As we hypothesized Carat, price, x, y and z are well explained by factor 1. Depth is very well explained by factor 2 but table is to a lesser degree. We see some evidence of the factor structure in our hypothesis with the "price + dimension" variables strongly implicated in Factor 1 and "Light conductance" variables most strongly explained by factor 2. However, we also see other original variables we did not expect in each of our factors. Factor 1 explains 66% of the variance in the data and Factor 2 explains an additional 16%. We also see high uniqueness ratings for table indicating that the two factors do not well account for it's variance. 

Overall the hypothesis test gives a highly significant result meaning that two factors is not sufficient to capture the full dimensionality of the data set.


# Cluster Analysis

```{r}
clusters <- kmeans(scale(smallersample[,-c((2:4),11)]), algorithm = "MacQueen", centers = 4, iter.max = 100, nstart = 50)
print(clusters)
```



```{r}
fviz_cluster(clusters, data = (scale(smallersample[,-c(2:4)])),
             geom = "point",
             ellipse.type = "norm", 
             ggtheme = theme_bw()
             )
```

```{r}
fviz_nbclust((scale(smallersample[,-c(2:4)])), kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```

# LDA 

```{r}
set.seed(300525287, kind = "Mersenne-Twister")
ind <- sample(c("Train", "Test"),
              nrow(smallersample),
              replace = TRUE, 
              prob = c(.6, .4))
Train <- smallersample[ind == "Train",]
Test <- smallersample[ind == "Test",]
(LDA <- lda(cut ~ carat + depth + table + price + x + y + z, data = Train))
```

```{r}
Pred <- predict(LDA)
ldahist(data = Pred$x[,1], g=Train$cut)
ggord(LDA, Train$cut)
partimat(cut ~ carat + depth + table + price + x + y + z, data = Train, method = "lda")
```

```{r}
RealisticPredicted <- predict(LDA, Test)$class
(RCM <- table(RealisticPredicted, Actual = Test$cut))
sum(diag(RCM))/sum(RCM)
confusionMatrix(RCM)
```


