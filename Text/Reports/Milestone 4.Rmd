---
title: "STAT394 Group Project Milestone 4"
author: "Ken MacIver, Tom Tribe, Jundi Yang, Mei Huang"
date: "`r Sys.Date()`"
classoption: 12pt
output: bookdown::pdf_document2
bibliography: ./MASTER.bib
header-includes: \usepackage{float}
                    \floatplacement{figure}{H}
                    \floatplacement{table}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE}
# load the required packages
require(ggplot2)
require(ggthemes)
library(ggstance)
library(ggcorrplot)
library(ggplot2)
library(mvtnorm)
library(fitdistrplus)
library(GGally)
library(ggExtra)
library(reshape2)
library(xtable)
library(moments)
library(psych)
library(Hotelling)
library(car)
library(HDtest)
library(ggpubr)
options(xtable.floating = FALSE)
options(xtable.timestamp = "")

```

# The 'diamonds' dataset

NOTE: The size of this dataset means that rendering to PDF takes a long time.

For the STAT394 Group Project, Group 11 have chosen a dataset called 'diamonds', which presents data on 53940 diamonds. It was accessed it from kaggle.com. There are ten variables:

-   carat: the diamond's weight (numerical: 0.2 - 5.01)
-   cut: a measure of quality (categorical: Fair, Good, Very Good, Premium)\
-   color: a measure of colour quality (categorical: J, which is poorest quality, to D, which is best)
-   clarity: a measure of clearness (categorical: from worst to best = I1, SI2, VS2, VS1, VVS2, IF)
-   x: length in mm (0 - 10.74)
-   y: width in mm (0 - 58.9) 
-   z: depth in mm (0 - 31.8)
-   depth: total depth percentage = z/mean(x,y) = 2\*z/(x+y) (43 - 79)
-   table: width of top of diamond relative to widest point
-   price: the price of the diamond in US dollars (List adapted from the list at @diamondskaggle). 

We are most interested in how these variables relate to and predict diamond price.

### Load the dataset into R.

```{r, echo=FALSE}

diamonds <- read.csv("./diamonds.csv", encoding = "UTF-8")
diamonds[1:5,]

# remove the index column
diamonds$X <- NULL

# get an overview of the structure of the data
str(diamonds)
```

## Create factor levels and view summary

```{r, echo=FALSE}
# set categorical variables as factors and set levels
diamonds$cut <- factor(diamonds$cut, 
                       levels = c("Fair","Good","Very Good","Premium","Ideal"))
diamonds$color <- factor(diamonds$color, 
                         levels = c("J","I","H","G","F","E","D"))
diamonds$clarity <- factor(diamonds$clarity,
                           levels = c("I1","SI2","SI1","VS2","VS1","VVS2","VVS1","IF"))

# make data frame of just the numerical variables
diamonds_num <- subset(diamonds, select = c(carat,depth,table,price,x,y,z))

# display first rows to check
diamonds_num[1:4,]

# display summary data
summary(diamonds_num)
```

#### Summary function diamonds

```{r, echo=FALSE}
# use the summary function to create the summary data for the numerical variables
MySummary <- function(x){
  return(c(
    length(x),
    min(x),
    quantile(x, .25),
    median(x),
    mean(x),
    quantile(x, .75),
    max(x),
    IQR(x),
    sd(x), 
    skewness(x),
    kurtosis(x)))
}

```

#### Change array rownames

```{r, echo=FALSE}
# store the summary data in a variable and modify row names for the output table
summ_diamonds <- apply(diamonds_num, MySummary, MARGIN=2)

rownames(summ_diamonds) <- c("sample size","minimum",
                                   "first quartile","median",
                                   "mean","third quartile",
                                   "maximum","IQR", "standard deviation",
                            "skewness","kurtosis")
```

### Summary table diamonds

```{r summtablediam}
# produce summary table
knitr::kable(signif(summ_diamonds,2), caption = "Summary statistics for 'diamonds' (2 s.f.)")
```

Table \@ref(tab:summtablediam) presents the summary statistics for the numerical variables in the diamonds dataset. 

### Means vector

```{r, echo=FALSE}
means_vec_diam <- matrix(colMeans(diamonds_num), 
                    nrow = length(colnames))
signif(means_vec_diam,4)

```

The estimates for the means vector are displayed in both the output above and in vector form below:

$$
{\bf \hat\mu}=\begin{pmatrix}
0.7979\\
61.75\\
57.46\\
3933\\
5.731\\
5.735\\
3.539
\end{pmatrix}
$$

## Multivariate Tests

Our categorical variables of cut, clarity and color are all ordinal:
-   cut: a measure of quality (categorical: Fair, Good, Very Good, Premium)\
-   color: a measure of colour quality (categorical: J, which is poorest quality, to D, which is best)
-   clarity: a measure of clearness (categorical: from worst to best = I1, SI2, VS2, VS1, VVS2, IF)

Using a Hotelling's T-test we will test the equality of mean vectors of the lowest and highest level of each categorical variable. We might expect the mean vector of diamonds with lower quality cut, color and clarity to differ significantly to diamonds with high quality cut, colour and clarity. 


```{r}
(hotelling.test(subset(diamonds, cut == "Fair")[,-(2:4)], subset(diamonds, cut == "Premium")[,-(2:4)]))

```

We may reject the null hypothesis that the mean vectors of these two samples are equal.

```{r}
(hotelling.test(subset(diamonds, color == "J")[,-(2:4)], subset(diamonds, color == "D")[,-(2:4)]))

```

We may reject the null hypothesis that the mean vectors of these two samples are equal.

```{r}
(hotelling.test(subset(diamonds, clarity == "I1")[,-(2:4)], subset(diamonds, clarity == "IF")[,-(2:4)]))

```

We may reject the null hypothesis that the mean vectors of these two samples are equal.

## Normality

In our previous EDA we saw that a number of our numeric variables may not follow a normal distribution. We saw this in the Cullen and Frey Plots and also in our summary table by examining the values for kurtosis and skewness. We will create a Normal QQ plot for each numeric variable as well as performing a Kolmogrov-Smirnov goodness of fit test for each numeric variable. The density function of each numeric variable indicate strong deviations from the normal distribution. 


```{r  fig.cap= "Density of Numeric variables", fig.pos= "center", fig.pos= "H"}
a <- ggplot(diamonds, aes(x=carat))+
  geom_density(color="darkblue", fill="lightblue") + xlab("Carat")

b <- ggplot(diamonds, aes(x=depth))+
  geom_density(color="darkblue", fill="lightblue") + xlab("Depth")

c <- ggplot(diamonds, aes(x=table))+
  geom_density(color="darkblue", fill="lightblue") + xlab("Table")

d <- ggplot(diamonds, aes(x=price))+
  geom_density(color="darkblue", fill="lightblue") + xlab("Price")

e <- ggplot(diamonds, aes(x=x))+
  geom_density(color="darkblue", fill="lightblue") + xlab("x")

f <- ggplot(diamonds, aes(x=y))+
  geom_density(color="darkblue", fill="lightblue") + xlab("y")

g <- ggplot(diamonds, aes(x=z))+
  geom_density(color="darkblue", fill="lightblue") + xlab("z")


ggarrange(a, b, c, d, e, f, g + rremove("x.text"), 
          labels = c("Carat", "Depth", "Table", "Price", "x", "y", "z"),
          ncol = 2, nrow = 4)
```     

Attempting a log transform of numeric variables to increase normality did not yield much success.

```{r  fig.cap= "Density of Numeric variables", fig.pos= "center", fig.pos= "H"}
a <- ggplot(diamonds, aes(x=log(carat)))+
  geom_density(color="darkblue", fill="lightblue") + xlab("Carat")

b <- ggplot(diamonds, aes(x=log(depth)))+
  geom_density(color="darkblue", fill="lightblue") + xlab("Depth")

c <- ggplot(diamonds, aes(x=log(table)))+
  geom_density(color="darkblue", fill="lightblue") + xlab("Table")

d <- ggplot(diamonds, aes(x=log(price)))+
  geom_density(color="darkblue", fill="lightblue") + xlab("Price")

e <- ggplot(diamonds, aes(x=log(x)))+
  geom_density(color="darkblue", fill="lightblue") + xlab("x")

f <- ggplot(diamonds, aes(x=log(y)))+
  geom_density(color="darkblue", fill="lightblue") + xlab("y")

g <- ggplot(diamonds, aes(x=log(z)))+
  geom_density(color="darkblue", fill="lightblue") + xlab("z")


ggarrange(a, b, c, d, e, f, g + rremove("x.text"), 
          labels = c("Carat", "Depth", "Table", "Price", "x", "y", "z"),
          ncol = 2, nrow = 4)
```     


### Carat

```{r,  fig.pos= "center", fig.pos= "H"}
qqnorm(diamonds$carat, xlab = "Observations", ylab = "Normal Quantiles", col = "red")
qqline(diamonds$carat, col = "blue", lwd =2)
```

```{r}
ks.test(diamonds$carat, "pnorm", mean=mean(diamonds$carat), sd=sd(diamonds$carat))
```

The QQ plot shows strong deviations from the normal distribution particularly at the tails and this is confirmed in our hypothesis test of normality.

```{r,  fig.pos= "center", fig.pos= "H"}
qqnorm(log(diamonds$carat), xlab = "Observations", ylab = "Normal Quantiles", col = "red")
qqline(log(diamonds$carat), col = "blue", lwd =2)
```

### Depth

```{r,  fig.pos= "center", fig.pos= "H"}
qqnorm(diamonds$depth, xlab = "Observations", ylab = "Normal Quantiles", col = "red")
qqline(diamonds$depth, col = "blue", lwd =2)
```

```{r}
ks.test(diamonds$depth, "pnorm", mean=mean(diamonds$depth), sd=sd(diamonds$depth))
```

Again the QQplot for depth shows substantial deviations from the normal distribution at the tails. The KS test finds evidence to reject the hypothesis that the data is normally distributed 

### Table

```{r,  fig.pos= "center", fig.pos= "H"}
qqnorm(diamonds$table, xlab = "Observations", ylab = "Normal Quantiles", col = "red")
qqline(diamonds$table, col = "blue", lwd =2)
```

```{r}
ks.test(diamonds$table, "pnorm", mean=mean(diamonds$table), sd=sd(diamonds$table))
```

### Price

```{r,  fig.pos= "center", fig.pos= "H"}
qqnorm(diamonds$price, xlab = "Observations", ylab = "Normal Quantiles", col = "red")
qqline(diamonds$price, col = "blue", lwd =2)
```

```{r}
ks.test(diamonds$price, "pnorm", mean=mean(diamonds$price), sd=sd(diamonds$price))
```

### x (length)

```{r,  fig.pos= "center", fig.pos= "H"}
qqnorm(diamonds$x, xlab = "Observations", ylab = "Normal Quantiles", col = "red")
qqline(diamonds$x, col = "blue", lwd =2)
```

```{r}
ks.test(diamonds$x, "pnorm", mean=mean(diamonds$x), sd=sd(diamonds$x))
```

### y (width)

```{r,  fig.pos= "center", fig.pos= "H"}
qqnorm(diamonds$y, xlab = "Observations", ylab = "Normal Quantiles", col = "red")
qqline(diamonds$y, col = "blue", lwd =2)
```

```{r}
ks.test(diamonds$y, "pnorm", mean=mean(diamonds$y), sd=sd(diamonds$y))
```

### z (depth)

```{r,  fig.pos= "center", fig.pos= "H"}
qqnorm(diamonds$depth, xlab = "Observations", ylab = "Normal Quantiles", col = "red")
qqline(diamonds$depth, col = "blue", lwd =2)
```

```{r}
ks.test(diamonds$depth, "pnorm", mean=mean(diamonds$depth), sd=sd(diamonds$depth))
```


All of our numerical variables, except perhaps y (width), show obvious deviation from the normal distribution in the QQ plots. The KS goodness of fit test finds evidence that these varaibles do not follow a normal distribution. 

## Melted version of dataset

```{r, echo=FALSE}
# create melted version of the dataset to allow easy graphical manipulation
diamonds_melt <- melt(data = diamonds, id.vars = c("cut","color","clarity"),
                      variable.name = "metrics")

```

## Boxplots of 'cut'

```{r bxpt1, fig.cap="Boxplots of 'cut' vs all the numeric variables", echo=FALSE}
ggplot(data=diamonds_melt, aes(x=metrics, y=value)) +
  
  geom_boxplot(aes(col=metrics), notch = TRUE) +
  facet_grid(~ cut) +
  theme(axis.text.x = element_text(size=7, angle=90, hjust=1),
        legend.position = "none")

```

Figure \@ref(fig:bxpt1) shows that all the variables except 'price' are too compressed to view. Therefore, I will perform a log transform and redo the graph.

## Boxplots of 'cut' in log scale
The summary of the dataset shows that there are no negative or zero values, so we can proceed with a log transform. 

```{r bxpt1log, fig.cap="Boxplots of 'cut' vs all the numeric variables (log transformed)", echo=FALSE}
ggplot(data=diamonds_melt, aes(x=metrics, y=value)) +
  
  geom_boxplot(aes(col=metrics), notch = TRUE) +
  scale_y_log10()+
  facet_grid(~ cut) +
  theme(axis.text.x = element_text(size=7, angle=90, hjust=1),
        legend.position = "none")

```

The log transform in Figure \@ref(fig:bxpt1log) gives a much better idea of the data. 'Price' (green boxes) is consistent across the different levels of 'cut'. Indeed, on this graph the medians and variances of all the variables look similar across the different levels of 'cut'. However, genuine differences might be difficult to perceive due to the scale of the graph and because the sample size is so large, meaning that a seemingly small difference on the graph could still be significant. Most of the confidence interval notches on the boxplots are too compressed to be of help. Below we conduct an ANOVA to determine if we have evidence of differences in price for different levels of the categorical variable cut.

For two of the variables (measurements 'y', purple, and 'z' pink) in the 'Very Good', 'Premium' and 'Ideal' levels of 'cut' there appear to be some very prominent outliers, as evidenced by the pink and purple dots above and below the boxplots. The variable 'y' is a measure of width in millimeters (mm), while 'z' is a measure of depth in mm. 

## Differences in Price for different levels of Cut

```{r Distribution of Price for different levels of Cut, echo = FALSE}
ggplot(subset(diamonds, cut == "Ideal"| cut == "Premium"| cut =="Good"| cut == "Very Good"| cut == "Fair"), aes(x = price, col = cut, group = cut, fill = cut)) +
  geom_density(aes(y = ..density..), alpha = .7)
```

We will now test whether there are significant differences in mean price for different levels of "cut"

```{r}
cutanova <- aov(price ~ cut, data = diamonds)
summary(cutanova)
```

We have strong evidence to suggest that mean price differs across levels of "cut". We will now perform a Tukey Test to determine which pairwise differences are significant

```{r}
TukeyHSD(cutanova, conf.level = 0.95)
```

The only pairs we do not see a significant difference (at a 5% significance level) in mean price between are "very Good" and "Good" as well as "Premium" and "Fair". 

From the graph of the distributions of price for different levels of cut we can see that not all of them have a shape consistent with being normally distributed. A one Way ANOVA is reasonably robust to departures from normality, particularly as we have a very large sample. We will also perform a levene test to test the assumption of equal variances and then, if significant, a non-parametric Kruskal Wallis test to determine if there are significant differences in median price for different levels of cut. Again, a one way ANOVA is reasonably robust to departures from equal variance if the sample sizes are the same.

```{r}
leveneTest(price ~ cut, data= diamonds)
kruskal.test(price ~ cut, data = diamonds)
```

Both tests return significant results indicating that the assumption of equal variance is violated and that we have evidence of a signifiant differnce in median prices for different levels of cut. 

## Boxplots of 'color'

```{r bxpt2, echo=FALSE}
ggplot(data=diamonds_melt, aes(x=metrics, y=value)) +
  
  geom_boxplot(aes(col=metrics), notch = TRUE) +
  facet_grid(~ color) +
  theme(axis.text.x = element_text(size=7, angle=90, hjust=1),
        legend.position = "none")

```

## Boxplots of 'color' in log scale
As with the 'cut' variable, we redo the boxplots using the log transform on the data. Figure \@ref(fig:bxpt2log) shows the log transformed version of the boxplots for the 'color' variable.

```{r bxpt2log, fig.cap="Boxplots of 'color' vs all the numeric variables (log transformed)", echo=FALSE}
ggplot(data=diamonds_melt, aes(x=metrics, y=value)) +
  
  geom_boxplot(aes(col=metrics), notch = TRUE) +
  scale_y_log10()+
  facet_grid(~ cut) +
  theme(axis.text.x = element_text(size=7, angle=90, hjust=1),
        legend.position = "none")

```

## Differences in Price for different colors

```{r Distribution of Price for different levels of Color, echo = FALSE}
ggplot(subset(diamonds, color == "J"| color == "H"| color =="I"| color == "G"| color == "F"| color == "E"| color == "D"), aes(x = price, col = color, group = color, fill = color)) +
  geom_density(aes(y = ..density..), alpha = .7)
```

We will now test whether there are significant differences in mean price for different diamond colours. While we see potential evidence that the ANOVA assumptions of normality and equal variance may be violated ANOVA is reasonably robust to these violations if the sample size is big enough. 

```{r}
coloranova <- aov(price ~ color, data = diamonds)
summary(cutanova)
```
We have strong evidence to suggest that mean price differs across levels of "color". We will now perform a Tukey Test to determine which pairwise comparisons are significant

```{r}
TukeyHSD(coloranova, conf.level = 0.95)
```

We see significant differences in mean price for nearly all pairwise comparisons of diamond colors.

We will also perform a levene test to test the assumption of equal variances and then, if significant, a non-parametric Kruskal Wallis test to determine if there are significant differences in median price for different levels of cut.

```{r}
leveneTest(price ~ color, data= diamonds)
kruskal.test(price ~ color, data = diamonds)
```

We find a significant difference in the median price for different colour diamonds. 


 

## Boxplots of 'clarity'
For the 'clarity' variable I have immediately performed a log transform on the data for the boxplots. 

```{r bxpt3log, fig.cap="Boxplots of 'clarity' vs all the numeric variables (log transformed)", echo=FALSE}
ggplot(data=diamonds_melt, aes(x=metrics, y=value)) +
  
  geom_boxplot(aes(col=metrics), notch = TRUE) +
  scale_y_log10()+
  facet_grid(~ clarity) +
  theme(axis.text.x = element_text(size=7, angle=90, hjust=1),
        legend.position = "none")

```

Figure \@ref(fig:bxpt3log) shows the boxplots for the log transformed data across the different 'clarity' metrics. Much like the previous two graphs, medians and ranges look relatively constant across the variables. 

## Differences in price for different levels of clarity

```{r Distribution of Price for different levels of Clarity, echo = FALSE}
ggplot(subset(diamonds, clarity == "I1"| clarity =="SI2"|clarity =="SI1"|clarity =="VS2"| clarity =="VS1"| clarity =="VVS2"| clarity =="VVS1"| clarity =="IF"), aes(x = price, col = clarity, group = clarity, fill = clarity)) +
  geom_density(aes(y = ..density..), alpha = .7)
```

We will now test whether there are significant differences in mean price for different diamond colours. While we see potential evidence that the ANOVA assumptions of normality and equal variance may be violated ANOVA is reasonably robust to these violations if the sample size is big enough. 

```{r}
clarityanova <- aov(price ~ clarity, data = diamonds)
summary(clarityanova)
```
We have strong evidence to suggest that mean price differs across levels of "clarity". We will no perform a Tukey Test to determine which pairwise comparisons are significant

```{r}
TukeyHSD(clarityanova, conf.level = 0.95)

```

We will also perform a levene test to test the assumption of equal variances and then, if significant, a non-parametric Kruskal Wallis test to determine if there are significant differences in median price for different levels of cut.

```{r}
leveneTest(price ~ clarity, data= diamonds)
kruskal.test(price ~ clarity, data = diamonds)
```


### The covariance matrix

```{r tabcovdia, echo=FALSE}
covar_diam <- cov(diamonds_num)
knitr::kable(signif(covar_diam,2), caption = "Covariance matrix for 'diamonds' (2 s.f.)")
```

The covariance matrix can be seen in table \@ref(tab:tabcovdia). 

### The correlation matrix

```{r tabcorrdiam, echo=FALSE}
correl_diam <- cor(diamonds_num)


knitr::kable(signif(correl_diam,2), caption = "Correlation matrix for 'diamonds' (2 s.f.)")
```

The correlation matrix can be seen in table \@ref(tab:tabcorrdiam).

## Visualisation of the correlation matrix

```{r correldiam, fig.cap="The pairs plot for diamonds", echo=FALSE}

ggcorrplot(cor(diamonds_num),
           method = "circle",
           hc.order = TRUE,
           type = "lower")
    
```

Figure \@ref(fig:correldiam) shows the correlation plot for the numerical variables in the diamonds data. 'Carat', 'x', 'y', 'z' and 'price' all show very strong correlations with each other, as evidenced by the large red dots. As "x", "y" and "z" are all measures of size we should expect this and there may be some redundancy in these predictors.  The 'table' variable is relatively uncorrelated with any of the others. 'Depth' and 'table' are negatively correlated (large purple dot), while depth is not correlated with any other variable. The strongest predictor of price is carat with length, width, depth also strongly correlated with price. Table is only very weakly correlated with price while depth is negatively correlated with price.




```{r pairsplot2, fig.cap="The pairs plot for Diamonds numeric values", fig.pos="center", echo=FALSE}
#the pairs plot
pairs.panels(diamonds_num, method = "spearman", 
              #hist.col = "lightgreen", 
              density = TRUE, 
              ellipses = FALSE)
```

Figure \@ref(fig:pairsplot2) shows the pairs plot for the numeric variables. While the scatterplots are small, it is clear that a number of pairs show little to no correlation, supporting the results from the correlation plot (figure \@ref(fig:correldiam)). 

## Scatterplots

Based on the outcome of the correlation pairs plot (figure  \@ref(fig:correldiam)) we have chosen the pairs 'depth' and 'table', and 'price' and 'carat' to produce scatter plots of, because one pair shows a strong positive correlation while the other shows a strong negative correlation.


```{r scatdiamdt, fig.cap="Scatterplot with marginal boxplots of 'depth' vs 'table'", echo=FALSE}
diam_deptab <- ggplot(diamonds, aes(x=depth, y=table))+
  geom_point()+
  coord_fixed()+
 
  labs(title = "Scatterplot with \nmarginal boxplots")
  
ggMarginal(diam_deptab, type = "boxplot", notch=TRUE, size = 15, 
           fill="grey")
```

Figure \@ref(fig:scatdiamdt) shows the scatterplot with marginal boxplots of the 'depth' and 'table' variables. Because of the large number of observations, some of the visualisation is compressed to the point where it is difficult to read, for example the outliers on the marginal boxplot along the top. In the above example, the effects ratio is fixed to allow easier visualisation of the negative correlation, but this has resulted in a horizontal compression.

We can see from the marginal boxplots (grey boxes along the top and right) that most of the datapoints are clustered tightly around the medians of both variables, causing an area in the middle of the scatterplot that is so dense at to be black. There are a few outliers for each variable, but not many considering that there are over 53,000 observations. The strong negative correlation that we saw in the pairs plot is reasonably visible as evidenced by the dark directional band going from top left toward the bottom right.


```{r scatdiamcp, fig.cap="Scatterplot with marginal boxplots of 'carat' vs 'price'", echo=FALSE}
diam_deptab <- ggplot(diamonds, aes(x=carat, y=price))+
  geom_point()+
  
 
  labs(title = "Scatterplot with marginal boxplots of 'carat' vs 'price'")
  
ggMarginal(diam_deptab, type = "boxplot", notch=TRUE, size = 15, 
           fill="grey")
```

Figure \@ref(fig:scatdiamcp) shows the scatterplot with marginal boxplots of the 'carat' and 'price' variables. The expected positive correlation is clearly visible as a dark band running from the bottom left steeply towards the top right. There are vertical bands of visible at the $1, 1.5~and~2$ values of carat. This is a curious finding and one that is an obvious point of investigation for a more comprehensive analysis. Perhaps jewelers are in the habit of rounding down to the nearest whole or half number, despite carat being a continuous variable? Another curious aspect is why the lower parts of those ranges (from 1.5 to 1.6, for example) are so densly packed with observations, while the upper parts (1.8 to 2) appear virtually empty. It seems very unlikely that by chance there were few stones of this weight, so presumably another factor is at play.


```{r scatdiamxz, fig.cap="Scatterplot with marginal boxplots of 'x' vs 'z'", echo=FALSE}
diam_deptab <- ggplot(diamonds, aes(x= diamonds$x, y= diamonds$z))+
  geom_point()+
  
 
  labs(title = "Scatterplot with marginal boxplots of 'carat' vs 'price'")
  
ggMarginal(diam_deptab, type = "boxplot", notch=TRUE, size = 15, 
           fill="grey")
```
Figure \@ref(fig:scatdiamxz) illustrates the strong positve correlation between the measurement dimensions of length and depth (x and z). The Correlation plot shows strong positive correlations between x (length in mm), y (width in mm) and z (depth in mm) indicating that there may be some redundanct between these variables.


## Mahalanobis Distance

We will examine and display surprising points in our dataset using the Mahalanobis Distance.

```{r}
diamonds_num$price <- as.numeric(diamonds_num$price)
mu.hat <- colMeans(diamonds_num)
sigma.hat <- cov(diamonds_num)
dM <- mahalanobis(diamonds_num, center = mu.hat, cov = sigma.hat)
upper.quantiles <- qchisq(c(.9,.95,.99), df = 7)
density.at.quantiles <- dchisq(x = upper.quantiles, df = 7)
cut.points <- data.frame(upper.quantiles, density.at.quantiles)
```

```{r}
diamonds_num$dM <- dM
diamonds_num$surprise <- cut(diamonds_num$dM, breaks = c(0, upper.quantiles, Inf), labels = c("Typical", "Somewhat", "Surprising", "very"))
table(diamonds_num$surprise)
```

We see that while the vast majority of diamonds are typical there are a reasonable amount of "Surprising" and "Very Surprising" points in this dataset.


$y = \alpha + \beta carat + \gamma cut + \tau color + \omega clarity + \epsilon$

```{r}
lm.milestone4 <- lm(y~carat+cut+color+clarity, data = diamonds, x = T)
summary(lm.milestone4)
#pchisq(lm.milestone4$, df=lm.milestone4$, lower.tail=FALSE)
```
