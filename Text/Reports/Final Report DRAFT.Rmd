---
title: "STAT394 Group Project Final Report"
author: "Ken MacIver, Tom Tribe, Jundi Yang, Mei Huang"
date: "`r Sys.Date()`"
classoption: 12pt
output: bookdown::pdf_document2
bibliography: ./Bibliography/MASTER.bib
header-includes: \usepackage{float}
                    \floatplacement{figure}{H}
                    \floatplacement{table}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(ggthemes)
library(ggstance)
library(ggcorrplot)
library(ggplot2)
library(mvtnorm)
library(fitdistrplus)
library(GGally)
library(ggExtra)
library(reshape2)
library(xtable)
library(moments)
library(psych)
library(Hotelling)
library(car)
library(HDtest)
library(ggpubr)
library(cowplot)
library(devtools)
library(ggbiplot)
library(factoextra)
library(devtools) # general package
library(ggord)
library(klaR)
library(cluster)
library(caret) 

options(xtable.floating = FALSE)
options(xtable.timestamp = "")
```

# Abstract

An exploratory data analysis (EDA) of the 'diamonds' dataset was undertaken. Based on the results of the EDA, the techniques of Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Factor Analysis (FA) and Cluster Analysis (CA) were carried out. As these techniques were new to the project team and everyone was keen to learn, it was decided to experiment with each of them (with varying degrees of success). 

The primary aim was to find the best predictor variables for 'price'. A secondary aim was to use the new techniques of LDA and CA to see if simple classifications could be made. Other techniques used were single and multiple linear regression and testing to find the best regression model using the Akaike Information Criterion (AIC) and Bayes Information Criterion (BIC) tests.


# Introduction  

One of the key skills required of statisticians is the ability to explore a new and unfamiliar dataset and allow that exploration to guide their next steps. A second key skill is knowing which analytical methods to subsequently apply to the dataset. As students enrolled in the Victoria University of Wellington year three Multivariate Statistics paper (STAT394), we were tasked with undertaking a group project designed to give us practice in developing these skills.

We selected the 'diamonds' dataset, which was unfamiliar to us all. 'Diamonds' is available in the ggplot2 package in RStudio and is also freely available on Kaggle [@diamondskaggle]. It contains information on ten different variables for 53940 diamonds. Of these ten variables three are categorical while seven are numerical. A full breakdown of the variables is given below. 

This project provided the opportunity to face the challenges of working with a new and unfamiliar dataset and to gain experience in trouble-shooting the many issues that were encountered.

## The variables

- Carat: a measure of the diamond's weight. One carat equals 1/5 gram. In this data set there are diamonds with carat values ranging from 0.2 - 5.01.  

- Cut: A diamond's cut defines its proportions and its ability to reflect light. This variable has five levels: Fair (lowest quality), Good, Very Good, Premium and Ideal (highest quality).  

- Color: A diamond's color refers to how clear/colorless it is. This variable has seven levels: J (lowest quality), I, H, G, F, E, D (highest quality).

- Clarity: measures small imperfections on the surface and within the stone. This variable has eight levels:  I1 (lowest clarity), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (highest clarity).  

- x: length in mm. In this data set there are diamonds with a length ranging from 0 - 10.74. 

- y: width in mm. In this data set there are diamonds with a width ranging from 0 - 58.9.  

- z: depth in mm. In this data set there are diamonds with a depth ranging from 0 - 31.8. 

- Depth: total depth percentage. This is calculated by dividing the total width by the total depth ($depth=\frac{2\times z}{(x+y)}$). Depth percentage impacts how light reflects of the diamond. In this data set we have diamonds with a depth percentage ranging from 43% - 79%.

- Table: The flat facet on the top of a diamond is called its table. Table is calculated by dividing table width by the total width. Table percentage impacts how light reflects of the diamond. In this data set there are diamonds with a table percentage ranging from 43% - 95%.

- Price: price of the diamond in United States dollars (USD). In this data set there are diamonds with a price ranging from $326 - $18823.

### Why 'diamonds'?

We selected the diamonds dataset because it was easy to understand what the variables were measuring. This was in contrast to some of the other ones we looked at which required domain specific familiarity, such as knowledge about biology or chemistry (or even worse, bio-chemistry!). This data set also allows us to undertake an investigation from which we glean insights about diamonds that have real-world application and use. 

### Aims

Our primary aim was to identify which variables best predictor the *price* of a given diamond. We hypothesised that increased diamond size ('x', 'y' and 'z') and weight ('carat') will be positively correlated with diamond price. We also expected to see increased prices for diamonds with higher levels of the categorical variables, all of which reflect the quality of the diamond. We were unsure how diamond depth and table percentage will relate to diamond price. 

We also sought to explore the techniques of Principal Component Analysis (PCA), Factor Analysis (FA), Linear Discriminant Analysis (LDA) and Cluster Analysis (CA) that we have learned in STAT394. The diamonds dataset has 280 unique possible interactions (5 x 7 x 8) between different levels of the categorical variables. Our secondary aim was to discover whether one of these techinques could classify the data in a simpler manner, or predict into which level of a category a new observation would fall. Encouraged by our lecturer, we adopted an approach of 'playing' with the techniques with the aim of learning as much as possible, even if a more experienced statistician could see that they were not the correct tools to achieve our aims. We wanted to learn by doing, regardless of whether the results of these methods were successful for our project or not. 

# Methodology

The diamonds data set was accessed via the kaggle.com website [@diamondskaggle] while all statistical analysis and reporting was completed using the computer software package R [@R-base], in RStudio [@RStudio] and using RMarkdown [@RMarkdown]. Github [@github] was used a repository for storing all relevant documents and code during this project.

Upon loading the data set into R we first used the following code to delete unnecessary columns, ensure categorical variables were treated as factors with set levels and created a subset data set containing only the numeric variables.

```{r}
# Read the data set into R
diamonds <- read.csv("./diamonds.csv", encoding = "UTF-8")
# Remove the index column
diamonds$X <- NULL
# Set categorical variables as factors and set levels
diamonds$cut <- factor(diamonds$cut, 
                       levels = c("Fair","Good","Very Good","Premium","Ideal"))
diamonds$color <- factor(diamonds$color, 
                         levels = c("J","I","H","G","F","E","D"))
diamonds$clarity <- factor(diamonds$clarity,
                  levels = c("I1","SI2","SI1","VS2","VS1","VVS2","VVS1","IF"))
# Make data frame of just the numerical variables
diamonds_num <- subset(diamonds, select = c(carat,depth,table,price,x,y,z))
```

When scaling numerical values in our data set we used the `scale()` function.  

When taking smaller samples from our data set to improve the interpretability of visual plots we used a seed consisting of either the student identification number of the team member who wrote the code or 1234567890, and the pseudo random number generator Mersenne Twister.

To begin with, we undertook an in-depth Exploratory Data Analysis (EDA) of the data. The key results from the EDA can be found in the results section below. 

# Results

## Consolidated Exploratory Data Analysis (EDA)

The initial EDA was submitted previously as part of intermediary Milestones 3 and 4 for STAT34 and is not reproduced in full here. Key findings included:

- most numerical variables did not follow a Normal distribution. Some followed the beta distribution and some were undefined (according to the Cullen-Frey plots).

- strong positive correlations between a number of the numeric variables ('price', 'carat', 'x', 'y' and 'z'), and a strong negative correlation between 'depth' and 'table'.

- some of the density plots of the numerical variables showing multiple modes (peaks) suggesting that the effects of one or more of the levels of the categorical variables were having a strong influence on the distribution. These multiple modes were even more apparent in the density plots of the log-transformed variables.

- there were more extreme values for the Mahalanobis distances than would be expected for Normally distributed data. 

In this section we present again some of the key findings from the consolidated Exploratory Data Analysis as well as findings discovered by team members subsequent to the previous submission of the EDA in Milestones 3 and 4. 

## Summaries of Data


```{r summtab, echo=FALSE}
# use the summary function to create the summary data for the numerical variables
MySummary <- function(x){
  return(c(
    length(x),
    min(x),
    quantile(x, .25),
    median(x),
    mean(x),
    quantile(x, .75),
    max(x),
    IQR(x),
    sd(x), 
    skewness(x),
    kurtosis(x)))
}


summ_diamonds <- apply(diamonds_num, MySummary, MARGIN=2)

rownames(summ_diamonds) <- c("sample size","minimum",
                                   "first quartile","median",
                                   "mean","third quartile",
                                   "maximum","IQR", "standard deviation",
                            "skewness","kurtosis")

knitr::kable(summ_diamonds,
             digits = 2,
             caption = "Summary statistics for 'diamonds' (2 d.p.)")
```

A brief look at the summary statistics of the numerical variables (table \@ref(tab:summtab)) shows that there are a range of different scales. We also see high skewness values for carat, price, y and z and high kurtosis values for all variables, particularly y and z. Both of these indicate non-normality of our numeric variables. It is also interesting to note that at least one diamond has a zero value for x, y and z. These values are further investigated in the "Outliers and Unusual Points" section below. 


### Summaries of the Categorical Variables

These summaries are included to emphasise the fact that the categorical variables do not have an even distribution across their different levels. In some cases, this unevenness is quite pronounced. 

### Tables of counts for the categorical variables

```{r colortab, echo=FALSE}
colortab <- table(diamonds$color)

knitr::kable(colortab, 
             caption = "Count of 'color'",
             col.names = c("Level","Count"))
```

Table \@ref(tab:colortab) shows the number of diamonds in each level of the 'color' variable. As a reminder, 'J' is the lowest level and 'D' is the highest. Note that the most of the diamonds are in the middle of the range of levels.

```{r cuttab, echo=FALSE}
cuttab <- table(diamonds$cut)

knitr::kable(cuttab, 
             caption = "Count of 'cut'",
             col.names = c("Level","Count"))
```

Table \@ref(tab:cuttab) gives a breakdown of the how many diamonds are in each level of the 'cut' variable. As a reminder, 'Fair' is the lowest level and 'Ideal' is the highest. We can see that most are in the 'Ideal', with a substantial number also in the 'Premium' and 'Very Good'. The distribution increases markedly as the quality of 'cut' increases. 

```{r clartab, echo=FALSE}
clartab <- table(diamonds$clarity)

knitr::kable(clartab, 
             caption = "Count of 'clarity'",
             col.names = c("Level","Count"))
```

Table \@ref(tab:clartab) shows the number of diamonds in each level of the 'clarity' variable. As a reminder, 'I1' is the lowest level and 'IF' is the highest. Note that most of the diamonds are toward the middle of the range of levels.  

### Distribution of Numeric Variables 

While examining histograms, Cullen and Frey Plots, skewness and kurtosis of the numerical variables, we observed evidence that the numerical variables were not normally distributed. The assumption of normality underlies many statistical procedures so we investigated this in more depth using Normal Quantile plots and goodness-of-fit tests. The code for these tests is included below.

```{r normalQQplots, fig.cap= "Normal QQ Plots of Numeric Variables", fig.pos= "center", fig.pos= "H", echo=FALSE}
par(mfrow=c(3,3))  
qqnorm(diamonds$carat, xlab = "Observations", 
       ylab = "Normal Quantiles", main = "Carat", col = "red")
qqline(diamonds$carat, col = "blue", lwd =2)
qqnorm(diamonds$depth, xlab = "Observations", 
       ylab = "Normal Quantiles", main = "Depth", col = "red")
qqline(diamonds$depth, col = "blue", lwd =2)
qqnorm(diamonds$table, xlab = "Observations", 
       ylab = "Normal Quantiles", main = "Table", col = "red")
qqline(diamonds$table, col = "blue", lwd =2)
qqnorm(diamonds$price, xlab = "Observations", 
       ylab = "Normal Quantiles", main = "Price", col = "red")
qqline(diamonds$price, col = "blue", lwd =2)
qqnorm(diamonds$x, xlab = "Observations", 
       ylab = "Normal Quantiles", main = "x", col = "red")
qqline(diamonds$x, col = "blue", lwd =2)
qqnorm(diamonds$y, xlab = "Observations", 
       ylab = "Normal Quantiles", main = "y", col = "red")
qqline(diamonds$y, col = "blue", lwd =2)
qqnorm(diamonds$depth, xlab = "Observations", 
       ylab = "Normal Quantiles", main = "z", col = "red")
qqline(diamonds$depth, col = "blue", lwd =2)
```

Despite the small size of the Normal QQ plots in figure \@ref(fig:normalQQplots), it is clear that each numeric variable deviates significantly from a normal distribution. We tested this using Kolmogorov-Smirnov goodness-of-fit test. The results from these tests are shown in the table below. 

```{r KSGOF, echo=FALSE}
KS_GOF <- data.frame(Variable=c("Carat","Depth","Table","Price","x","y","z"),
                     Test_Statistic=c(0.12274, 0.075871, 0.13225, 0.18467,
                                         0.093545, 0.088528, 0.089273),
                     p_value=c(rep("<2.2e-16",7)))

knitr::kable(KS_GOF, 
             caption = "Kolmogorov-Smirnov GOF results")
                     
```

Table \@ref(tab:KSGOF) shows that for all seven of the numeric variables, the Kolmogorov-Smirnov goodness-of-fit tests provided strong evidence for non-normality, as evidenced by the p-values which are all machine precision zero.  

These tests supported our earlier observation that the data does not follow a normal distribution. We note this and proceed with our investigation anyway. 

### Correlation Plot

```{r corrplot, fig.cap="A Visualization of the Correlation Matrix", echo=FALSE, warning=FALSE}
ggcorrplot(cor(diamonds_num),
           method = "circle",
           hc.order = TRUE,
           type = "lower")
```

Figure \@ref(fig:corrplot) shows the correlation plot for the numerical variables in the diamonds data. 'Carat', 'x', 'y', 'z' and 'price' all show very strong correlations with each other, as evidenced by the large red dots. As "x", "y" and "z" are all measures of size we should expect this and there may be some redundancy in these predictors.  The 'table' variable is relatively uncorrelated with any of the others. 'Depth' and 'table' are negatively correlated (large purple dot), while depth is not correlated with any other variable. The strongest predictor of price is carat with length, width, depth also strongly correlated with price. Table is only very weakly correlated with price while depth is negatively correlated with price.

### Correlation Matrix

```{r cormat, echo=FALSE}
cordiam <- cor(diamonds_num)

knitr::kable(cordiam,
             digits = 3,
             caption = "Correlation matrix for the numerical variables")
```


Table \@ref(tab:cormat) shows the correlation matrix. The strong positive correlations between price and 'carat' (0.922), price and 'x' (0.884), price and 'y' (0.865) and price and 'z' (0.861) supported one of our starting hypotheses, namely that these dimension variables would be positively correlated with 'price'. We noticed that they were also highly correlated with each other (all values greater than 0.9), a fact we used subsequently to combine them into one of the factors for the Factor Analysis.


### Price differences across Categorical Variables

As our leading question is investigating which variables are most predictive or price we decided to investigate if there are significant differences in price across levels of each categorical variable. 

#### Cut  
\hfill\break  
Here we get the mean price of diamonds of different levels of the 'cut' variable. 


```{r cutmean, echo = FALSE}
var_means <- tapply(diamonds$price, diamonds$cut, mean)

knitr::kable(var_means, 
             digits = 0,
             col.names = "Mean Price",
             caption = "Mean prices for the five levels of 'cut'")
```


The mean price of diamonds differs for different levels of 'cut' (table \@ref(tab:cutmean)). However, the higher levels of cut do not necessarily lead to higher prices.

```{r dencut2, echo = FALSE, fig.cap="Densities of 'price' for different levels of 'cut'"}
ggplot(subset(diamonds, cut == "Ideal"| cut == "Premium"| cut =="Good"| cut == "Very Good"| cut == "Fair"), aes(x = price, col = cut, group = cut, fill = cut)) +
  geom_density(aes(y = ..density..), alpha = .7)
```

Figure \@ref(fig:dencut2) shows the density plots for the different levels of the 'price' variable. The legend on the right shows the level names and their order ('fair' = poorest, 'ideal' = best). Of note is the fact that the two best leves (premium and ideal) have significant peaks near the lower end of the price range compared with the other three. This is somewhat surprising, as intuitively one would imagine price to increase as the quality of cut increases. 

We conducted a Analysis of Variance (ANOVA) for all three of the categorical variables to see if the observed differences in mean price across the various levels were significant. We also conducted a Levene's test to verfiy if the assumption of homogeneity of variance was violated.  If these tests showed violations of the ANOVA assumptions, we then implemented a non-parametric Kruskal Wallis Test. From the graph of the distributions of price for different levels of 'cut' we can see that not all of them have a shape consistent with being normally distributed. A one-way ANOVA is reasonably robust to departures from normality, particularly as we have a very large sample. The code for these tests may be found in the appendices. 

```{r cutanotab, echo=FALSE}
cutanovatab <- data.frame(Test=c("ANOVA","Levene's Test","Kruskal Wallis"),
                      Test_Statistic=c(175.7, 123.6, 978.62),
                      p_value=c(rep("<2.2e-16",3)))
knitr::kable(cutanovatab,
             caption = "ANOVA of Price by Cut")
```

Table \@ref(tab:cutanotab) shows the results of the ANOVA, Levene's and Kruskal Wallis tests for the 'cut' variable. The tests are to check whether there are differences in price between the different levels of 'cut'. The ANOVA test returned a p-value of < 2.2e-16 (machine precision zero), meaning that there is strong evidence to suggest that mean price differs across levels of "cut". 

The Tukey Test indicated that at the 5% significance level, the only pairs between which we do not see a significant difference in mean price are 'Very Good' and 'Good' as well as 'Premium' and 'Fair'. Both the Levene's test and Kruskal Wallis Test return significant results  indicating that: a) the assumption of equal variance is violated and; b) that we have evidence of a significant difference in median prices for different levels of cut.  

#### Clarity   
\hfill\break    
Here we get the mean price of diamonds of different levels of the 'clarity' variable.  


```{r clarmean, echo = FALSE}
var_means_clarity <- tapply(diamonds$price, diamonds$clarity, mean)

knitr::kable(var_means_clarity, 
             digits = 0,
             col.names = "Mean Price",
             caption = "Mean prices for the 8 levels of 'clarity'")
```

Table \@ref(tab:clarmean) shows that the mean price of diamonds varies across different levels of 'clarity'. Interestingly the diamonds with the two highest clarities show the lowest mean price. 


```{r denclar2, echo = FALSE, fig.cap="Densities of 'price' for different levels of 'clarity'"}
ggplot(subset(diamonds, clarity == "I1"| clarity =="SI2"|clarity =="SI1"|clarity =="VS2"| clarity =="VS1"| clarity =="VVS2"| clarity =="VVS1"| clarity =="IF"), aes(x = price, col = clarity, group = clarity, fill = clarity)) +
  geom_density(aes(y = ..density..), alpha = .7)
```

Figure \@ref(fig:denclar2) shows the densities of the different levels of 'clarity' for price. A prominent peak is visible near the left for the better quality levels (IF, best, pink; WS1 second best, purple; WS2, third best, blue). 

```{r claranotab, echo=FALSE}
claranovatab <- data.frame(Test=c("ANOVA","Levene's Test","Kruskal Wallis"),
                      Test_Statistic=c(215, 77.809, 2718.2),
                      p_value=c(rep("<2.2e-16",3)))
knitr::kable(claranovatab,
             caption = "ANOVA of Price by Clarity")
```

Table \@ref(tab:claranotab) shows the table of results from the three tests on the variable 'clarity'. The output from the ANOVA returns a p-value of < 2.2e-16, meaning there is strong evidence to suggest that mean price differs across levels of 'clarity'. The output of the Levene's test and Kruskal Wallis tests show that there is not equal variances between the levels, and that there is a significant difference between different levels of clarity. A Tukey test showed that most pairwise combinations show a significant difference. There are only six that do not show a difference and they are: SI1-I1; VS2-I1; VS1-I1; VS2-SI1; VS1-VS2; and IF-VVS1.


#### Color  
\hfill\break
Here we get the mean price of diamonds of different levels of the 'color' variable.  

```{r colmean, echo = FALSE}
var_means_color <- tapply(diamonds$price, diamonds$color, mean)

knitr::kable(var_means_color, 
             digits = 0,
             col.names = "Mean Price",
             caption = "Mean prices for the 8 levels of 'color'")
```

Table \@ref(tab:colmean) shows the mean prices for the different levels of the 'color' variable and it is clear that they vary. The diamonds with the two highest color quality show the lowest mean price.


```{r densitycol, echo = FALSE, fig.cap="Densities of 'price' for the different levels of 'color'", echo = FALSE}
ggplot(subset(diamonds, color == "J"| color == "H"| color =="I"| color == "G"| color == "F"| color == "E"| color == "D"), aes(x = price, col = color, group = color, fill = color)) +
  geom_density(aes(y = ..density..), alpha = .7)
```

Figure \@ref(fig:densitycol) shows the density plots of 'price' for the different levels of the 'color' variable. There is a prominent peak on the left for the better quality levels of color (D in pink, E in purple, and F in blue), but otherwise the densities appear to be fairly similar. 

```{r colanotab, echo=FALSE}
colanovatab <- data.frame(Test=c("ANOVA","Levene's Test","Kruskal Wallis"),
                      Test_Statistic=c(175.7, 219.12, 1335.6),
                      p_value=c(rep("<2.2e-16",3)))
knitr::kable(colanovatab,
             caption = "ANOVA of Price by Color")
```

Table \@ref(tab:colanotab) shows the results of the tests for the 'color' variable. We have strong evidence to suggest that mean price differs across levels of 'color'. The output of the Levene's test and Kruskal Wallis test above show that the variances are not equal between the levels, and that there is a significant difference in median price across different levels of 'color'. The output from the Tukey Test showed significant differences in mean price for nearly all pairwise comparisons of diamond colors.

### Outliers and Unusual Points 

#### Mahalanobis Distance  
\hfill\break
We decided to investigate outliers and unusual points in our dataset. These points may have increased leverage or influence when we come to using variables to predict price.  

We began by using the Mahalanobis Distance to identify surprising and very surprising points. A 'somewhat surprising' point is one that is equal or greater than the 90th percentile, a 'surprising' point is one that is equal or greater than the 95th percentile, while a 'very surprising' point is one that is equal or greater than the 99th percentile. A 'typical' point is one that lies within the 90th percentile. 


```{r, echo = FALSE}
diamonds_num$price <- as.numeric(diamonds_num$price)
mu.hat <- colMeans(diamonds_num)
sigma.hat <- cov(diamonds_num)
dM <- mahalanobis(diamonds_num, center = mu.hat, cov = sigma.hat)
upper.quantiles <- qchisq(c(.9,.95,.99), df = 7)
density.at.quantiles <- dchisq(x = upper.quantiles, df = 7)
cut.points <- data.frame(upper.quantiles, density.at.quantiles)
diamonds_num$dM <- dM
diamonds_num$surprise <- cut(diamonds_num$dM, breaks = c(0, upper.quantiles, Inf), labels = c("Typical", "Somewhat Surprising", "Surprising", "Very Surprising"))
mahal_table <- table(diamonds_num$surprise)
```

```{r Maltab, echo=FALSE}
knitr::kable(mahal_table, 
             col.names = c("Mahal Dist","Count"),
             caption = "Mahalanobis distances surprising values")
```

We see from table \@ref(tab:Maltab) while the vast majority of diamonds are typical in terms of their Mahalanobis distances, there are a reasonable amount of "Surprising" and "Very Surprising" points in this dataset. We can see how many very surprising points (as identified with the Mahalanobis Distance) are members of each level of our categorical variables. This might help to indicate if any classes contain more surprising points than others.

```{r, echo = FALSE}
diamonds$surprise <- diamonds_num$surprise <- cut(diamonds_num$dM, breaks = c(0, upper.quantiles, Inf), labels = c("Typical", "Somewhat", "Surprising", "Very"))

VSdiamonds <- subset(diamonds, surprise =="Very")

```

```{r mahalcut, echo=FALSE}
mahalcut <- table(interaction(VSdiamonds$cut))

knitr::kable(mahalcut, 
             col.names = c("Mahal Dist","Count"),
             caption = "Mahalanobis distances for 'cut'")
```

```{r}

```


Table \@ref(tab:mahalcut) shows the 'very surprising' Mahalanobis distances for the different levels of the variable 'cut'. 


```{r mahalclar, echo=FALSE}
mahalclar <- table(interaction(VSdiamonds$clarity))

knitr::kable(mahalclar, 
             col.names = c("Mahal Dist","Count"),
             caption = "Mahalanobis distances for 'clarity'")
```

Table \@ref(tab:mahalclar) shows the 'very surprising' Mahalanobis distances for the different levels of the variable 'cut'. 


```{r mahalcol, echo=FALSE}
mahalcol <- table(interaction(VSdiamonds$color))

knitr::kable(mahalcol, 
             col.names = c("Mahal Dist","Count"),
             caption = "Mahalanobis distances for 'color'")
```

Table \@ref(tab:mahalcol) shows the 'very surprising' Mahalanobis distances for the different levels of the variable 'color'. 


#### Zero Values 
\hfill\break
In this section we identify any zero values and determine whether or not they are errors. The code below shows the lowest values for each of the seven numerical variables. The number of elements to display was tweaked until the output showed all the zero values (if any). This was partially to discover whether the extreme values in the dataset were having undue influence on results. 

```{r}
sort(decreasing=F, diamonds$carat)[1:10]
sort(decreasing=F, diamonds$x)[1:10]
sort(decreasing=F, diamonds$y)[1:10]
sort(decreasing=F, diamonds$z)[1:22]
sort(decreasing=F, diamonds$depth)[1:10]
sort(decreasing=F, diamonds$table)[1:10]
sort(decreasing=F, diamonds$price)[1:10]
```

The output above shows that x, y and z all have a number of zero values, which presumably are errors.

## Idendifying whether the zero observations are errors

We firstly make a dataframe version of the dataset to allow easy subsetting. Note that this version includes the 'surprising' Mahalanobis distance values (column name 'surprise'). This is useful as it allows us to see which erroneous values have generated very large Mahalanobis distances. 

```{r}
diamonds.df <- data.frame(diamonds)

```

We now display the zero values for each of the variable, noting whether the other variables are consistent with zero readings, or whether they indicate that this is impossible. For example, the diamond in row 1 below with zero mm length ('x'), but 6.62 mm width ('y').

```{r}
# display the zero values for 'x'
diamonds.df[diamonds$x==0,]
```


```{r}
# display the zero values for 'y'
diamonds.df[diamonds$y==0,]
```

```{r}
# display the zero values for 'z'
diamonds.df[diamonds$z==0,]
```

The outputs above show that the zero values for 'x', 'y' and 'z' are all errors. We can tell this because these diamonds have carat, depth and price values, meaning that they cannot have zero length (x), width (y) and depth (z). Note that for a lot of the observations with zero values in one of these variables also have zero values in the others. The variable 'y' is a good example; the output shows that every observation with a zero value for 'y' also has zero values for 'x' and 'z'.  Note also that all of the zero values have 'very surprising' Mahalanobis distance results. This suggests that they are exerting undue influence or leverage on our results. However, this might be counterbalanced to a degree by the very large number of observations in the dataset. 

### Upper-value outliers

The outputs below show the upper values for the seven numerical variables. This output guides further investigation as to which are likely to be genuine and which are probably errors. 

```{r}
# show ten largest values for each of the seven numerical variables
sort(decreasing=T, diamonds$carat)[1:10]
sort(decreasing=T, diamonds$x)[1:10]
sort(decreasing=T, diamonds$y)[1:10]
sort(decreasing=T, diamonds$z)[1:10]
sort(decreasing=T, diamonds$depth)[1:10]
sort(decreasing=T, diamonds$table)[1:10]
sort(decreasing=T, diamonds$price)[1:10]
```

The output above shows the ten largest values for each of the seven numerical variables. At a glance (informal inference) it appears that 'carat', 'y', 'z' and 'table' all have one or more values that are considerably higher than the rest, with y and z having maximums that are so extreme it is worth considering whether they are erroneous.   

## Determining whether the outliers are errors

In this section we go through the variables one at a time to determine whether the upper values are likely to be erroneous or genuine. 

### The 'x' variable: probably no upper value errors

```{r}
# display the values of 'x' that are greater than or equal to 10
diamonds.df[diamonds$x>=10,]

```

The output above shows that the largest 'x' values are not outrageously larger than the rest. Additionally, the values of the other variables for the largest 'x' value are reasonably aligned in terms of magnitude, suggesting that these large 'x' values are genuine.

### The 'y' variable: probably 2 upper value errors

```{r}
# display the values of 'x' that are greater than or equal to 11
diamonds.df[diamonds$y>=10,]

```

The output above suggests that the two extreme values for 'y' are almost certainly errors. They are 58.90	and 31.80; both are far larger than the next highest value, which is 10.54. It is very unlikely that diamonds with such enormous width values do not also have extreme length and depth values, or that they did not sell for more money. 

### The 'z' variable: probably 2 upper value errors

```{r}
diamonds.df[diamonds$z>=8,]

```

Similarly with the extreme value of 'z' (31.80). The other dimensions of this diamond do not tally with the extreme depth value, nor does the relatively low price. The next highest value has been included for comparison (8.06). This is almost certainly an error.

### The 'carat' variable: probably no upper value errors

```{r}
diamonds.df[diamonds.df$carat>4,]
```

The output above shows that the highest value for 'carat' (5.01) is not outrageously higher than the next highest (4.50), suggesting that this is a genuine value. Also, the other variables are comparible between the highest and second highest, so it is likely that this value can be trusted. 

### The 'table' variable: probably no upper value errors

```{r}
# display the values of 'table' that are greater than or equal to 73
diamonds.df[diamonds.df$table>=73,]
```

The output above for the highest values of 'table' show that the diamond with the largest value (95) was also considerably larger in other ways and was much more expensive. This suggests that this is a genuine value, rather than an error.  

Note that all the upper outliers above, whether genuine or erroneous, have generated 'very surprising' values for their Mahalanobis distances.

### 'depth' and 'price': probably no upper value errors

```{r}
sort(diamonds.df$depth, decreasing = TRUE)[1:10]
sort(diamonds.df$price, decreasing = TRUE)[1:10]
```

The output above shows that neither 'depth' nor 'price' have any extreme values. We can probably safely conclude that these are all genuine upper values.

## Colour-coded scatterplots

In these scatterplots we see the three categorical variables of 'cut', 'color' and 'clarity' all versus 'price', but further colour-coded so we can see the price of diamonds at the different levels of these variables. This exercise is useful to see why our subsequent linear regression model using the single predictor of 'carat' did not work as well as hoped. This is expanded upon in the commentary below under the heading "Why prediction without the categorical variables is not working well". 

### Scatterplot of 'Carat' vs 'Price', colour-coded by 'Cut'

```{r caratcut, fig.cap="Carat vs Price, coloured by Cut",  echo= FALSE}

# scatterplot of carat vs price, but also colouring the observations
# by 'cut'
ggplot(data=diamonds, aes(x=carat, y=price, color=cut))+
  geom_point()+
  guides(colour=guide_legend(reverse = T))
```

Figure \@ref(fig:caratcut) shows the scatterplot of 'carat' versus 'price' and coloured by 'cut'. Notice the amount of overlap between diamonds from different levels of 'cut'. There is no clear separation, meaning that a discriminant analysis or cluster analysis will struggle to distinguish between the levels.

### Scatterplot of 'Carat' vs 'Price', colour-coded by 'Clarity'

```{r caratclarity, fig.cap="Carat vs Price coloured by Clarity", echo = FALSE}

# scatterplot of carat vs price, but also colouring the observations
# by 'cut'
ggplot(data=diamonds, aes(x=carat, y=price, color=clarity))+
  geom_point()+
  guides(colour=guide_legend(reverse = T))
```


Figure \@ref(fig:caratclarity) shows the scatterplot of 'carat' versus 'price' and coloured by 'clarity'. In terms of separation of the different levels, this appears more promising. Note the clear colour bands running from the bottom left to the upper right. While not perfectly separated, we can see a clear orange band at the bottom, then a clearish yellow/brown band above that, and then a clear green band etc. However, these bands still run diagonally through a large range of 'price' values (for example, the band of crimson dots at the very left runs upward from a few hundred dollars all the way up past the 17,000 dollar mark), meaning that predicting price based solely on these levels will be problematic.


### Scatterplot of 'Carat' vs 'Price', colour coded by 'Color'

```{r caratcolor, fig.cap="Carat vs Price coloured by Color", echo=FALSE}

# scatterplot of carat vs price, but also colouring the observations
# by 'cut'
ggplot(data=diamonds, aes(x=carat, y=price, color=color))+
  geom_point()+
  guides(colour=guide_legend(reverse = T))
```


Figure \@ref(fig:caratcolor) shows the scatterplot of 'carat' versus 'price' and coloured by 'color'. 

In the three scatterplots figures \@ref(fig:caratcut), \@ref(fig:caratclarity) and \@ref(fig:caratcolor) we see a clear trend of lighter diamonds (lower 'carat' values) that are most expensive (so, in the upper left of the plot) having the highest quality of 'cut', 'clarity' and 'color'. This is no surprise, as we would expect the most expensive lighter diamonds to be of the best quality. 

Conversely, we can also see some heavier diamonds which are of lower quality and lower price. Note in figure \@ref(fig:caratcut) that some of the lowest quality diamonds in terms of 'cut' (orange dots in the centre of the plot, on the vertical value of 2) are relatively cheap, despite being heavier than some of the lighter more expensive diamonds. But eventually the weight of the diamonds drives the price up regardless of the quality of 'cut', as we can see from the three orange dots in the top right of the plot. These are of the lowest quality cut, but are the three heaviest diamonds in the dataset, and so end up being expensive.

### Interactions 

As we have 3 categorical variables: color, clarity and cut with 7, 8 and 5 levels respectively we have 280 separate interactions in our data set. This makes it likely that there will be unknown classes in our data. 

# Simple and Multiple Regression 

As we are interested in predicting price we thought it would be appropriate to do an initial multiple regression including all variables to see which are thought to be significant in predicting price when all other variables are included in the model. We have done this firstly with the raw variables and then with scaled versions of the variables.

## Simple linear regression using 'carat'

```{r, echo=FALSE, results='hide'}
carpricecor <- signif(cor(diamonds$carat, diamonds$price),4)

```

We theorised that 'carat' would be a very good predictor of 'price' based on the fact that 'carat' was the variable most highly correlated with 'price', with a correlation of `r carpricecor` between the two variables. Here we fit a simple linear regression model using 'carat' as the sole predictor variable. 

```{r}
# fit the linear regression model
carat.lm <- lm(price~carat, data = diamonds, x=T)

# display summary
summary(carat.lm)

# get anova table of linear regression model
anova(carat.lm)
```

```{r, echo=FALSE, results='hide'}
# sandpit for extracting values from model. will not show in rendered pdf
carlmsumm <- summary(carat.lm)
caranova <- anova(carat.lm)
carlmsumm[[10]]
carlmsumm[[11]]

F_stat <- caranova[[4]][1]
F_stat

caranova
caranova[[5]][1]

```

The output from the linear regression model above shows that 'carat' is a good predictor of 'price'. The t-test statistic is `r F_stat`, which is an enormous number, and the p-value is `r caranova[[5]][1]`. So, with a p-value of zero, we have very strong evidence to say that 'carat' is a good predictor of 'price'. 

## Testing to find the best regression model

### The Akaike Information Criterion (AIC) test
We now use the Akaike Information Criterion (AIC) stepwise regression test in R to find the best regression model that has 'price' as the response variable. We are particularly interested in whether any other models outperform the simple 'carat' model. The output for these tests is pages long, so for brevity the `results='hide'` command has been used in the code chunk to allow viewing of the code while not displaying the output.  

```{r}
# create version with the 'surprising' variable (Mahalanobis distance) removed
diamonds_reg <- diamonds[,-11]

```



```{r, results='hide'}
# show code, but don't display results as they run to pages long
# fit the linear regression model with all predictors
full.lm <- lm(price ~ .,data = diamonds_reg)
```

```{r, results='hide'}
# show code, but don't display results as they run to pages long
# display the model summary
summary(full.lm)

# use the step() function to perform the stepwise AIC test
step(full.lm, direction = "both")
```

Based on the output from the stepwise AIC regression comparison above, the best model which balances accuracy against parsimony is the one with predictors carat + cut + color + clarity + depth + table + x + z. In other words, the only predictor that was dropped is 'y'. The AIC value for this model is 758425, which is the same for the model that additionally drops 'z'. Therefore, following the principle of parsimony, we select the simplest model, which means selecting the one that drops the variables 'y' and 'z'. This model is: price = carat + cut + color + clarity + depth + table + x.

### Bayes Information Criterion (BIC) model comparison

As a check, we also perform a model comparison using the BIC criteria. 

```{r, results='hide'}
# show code, but don't display results as they run to pages long
# store the sample size in a variable 
n <- length(diamonds$price)

# repeat the step test with the added 'k' argument to perform the BIC
step(full.lm, direction = "both", k=log(n))
```

The model comparison using the Bayes Information Criterion (BIC) confirms that the best model is the one with variables carat + cut + color + clarity + depth + table + x. The BIC value for this model is 758621, compared with 758629 for the next best (which includes 'z'). 

## Fitting the best model

The 'best' model (as found by the AIC test above) is fitted below.

```{r}
best.lm <- lm(price ~ carat + cut + color + clarity +
                depth + table + x, data = diamonds)
summary(best.lm)

```

The summary output for the 'best' model above shows how the categorical variables, all of which measure diamond quality, add a lot to the accuracy of the model. For example, 'Clarity', which has 8 levels, adds from 2702.077 (lowest quality clarity) to 5344.338 (highest quality clarity) to price depending on which level the diamond is categorised at. This is a wide range and cannot be obtained with these categorical variables dropped from the model.

## Check Regression Assumptions
\hfill\break

Here we check the regression assumptions by examining the diagnostic plots.

```{r diagplots, fig.cap="Diagnostic plots to check regression assumptions", echo=FALSE}
par(mfrow=c(2,2))
plot(best.lm, 1)
plot(best.lm, 2)
plot(best.lm, 3)
plot(best.lm, 4)
```

By examining the residual plots (figure \@ref(fig:diagplots)) we see that it is likely that the assumptions underlying a multiple linear regression such as linearity, homogeneity of the variance and normality of the residuals are violated. There also appears to be a number of points with high leverage and influence. We should therefore be cautious in concluding much from this regression model. 

## Scaled multiple regression model

In this version we scale the numerical variables and include the three categorical variables in the model. 

```{r}
diamondslm <- lm(price ~ scale(carat) + scale(x) + scale(y) + 
                   scale(z) + scale(depth) + scale(table) + 
                   cut + clarity + color, data = diamonds)
summary(diamondslm)
```

This multiple regression indicates that all variables but y and z are significant in the model (once the effect of the other predictors has been accounted for). This model accounts of 91.98% of the variation in the data.  


# Principal Component Analysis

We will perform a PCA with the aim of being able to explain the variation in the data set with fewer dimensions. We also hope to then create a parsimonious model for predicting diamond price through principal components regression.


```{r, echo=FALSE}
pca.diamonds <- prcomp(subset(diamonds.df, select = c(1,5:10)),
                       center=TRUE, scale. = TRUE, retx=TRUE)
```

```{r, echo=FALSE}
pca.var <- pca.diamonds$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100,1)

```

```{r pcascree, fig.cap="Scree plot of the Principal Components", echo=FALSE}
barplot(pca.var.per, names.arg = pca.var.per,
        xlab = "Principal Components 1 to 7",
        ylab = "% information retained by each PC",
        main="Screeplot of the Principal Components")

```

Figure \@ref(fig:pcascree) shows the scree plot for the Principal Components of the diamonds dataset. PC1 contains `r pca.var.per[1]`% of the variance, PC2 contains `r pca.var.per[2]`%, while PC3 contains `r pca.var.per[3]`%. Between them they contain `r sum(pca.var.per[1:3])`%, which is clearly very good. Components beyond the third principal component explain minimal variance (less than 4%). This suggests we can describe the data using only the first two principal components without losing much information. We will now examine which variables contribute most strongly to each Principal Component with a particular focus on the first two. 


```{r}
pca.diamonds$rotation
```

Looking at the eigenvectors we see that PC1 is primarily defined by carat, price, x, y and z. PC1 seems to be defined by dimension variables and, as we saw in the correlation matrix, price is closely associated with these variables. The variables with the most weight in PC2 are depth and table. We see a similar pattern in PC3 with depth and table having the largest weighting. This makes sense as in our examination of correlations between variables we saw that price, x, y, z and carat were all strongly correlated. The opposite loadings of depth and table in PC2 and PC3 reflect there negative correlation. A visualization of the eigenvectors of the first two principal components is shown in figure \@ref(fig:eigenvectorplotpc)

```{r, eigenvectorplotpc, echo=FALSE}
Rotation.df <- data.frame(t(pca.diamonds$rotation))
Rotation.df$PC <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7")
Rotation.melt <- melt(Rotation.df, id.vars = "PC")
PC1eigen <- ggplot(Rotation.melt[c(1,8, 15,22,29,36,43),]) +
  geom_line(aes(x = variable, y = value, group = PC, col = PC), size = 3)
PC2eigen <- ggplot(Rotation.melt[c(2,9, 16,23,30,37,44),]) +
  geom_line(aes(x = variable, y = value, group = PC, col = PC), size = 3)
ggarrange(PC1eigen, PC2eigen, ncol =1, nrow =2)
```


A biplot of the data projected over the first two principal components is shown below.


```{r biplotpc, fig.cap="Biplot of Principal Components", echo=FALSE}
ggbiplot(pca.diamonds, obs.scale =1, var.scale = 1)
```

The large number of observations in the biplot of PC1 versus PC2 in figure \@ref(fig:biplotpc) and consequent overlapping of the dots makes interpretation difficult. We will take a smaller sample of 1000 from the diamonds data and redo the PCA after first checking that the smaller sample has similar properties to the full sample. Also of note in this biplot are two significant outliers to the far right, which are extreme values of PC1. We shall identify those before creating the smaller sample.

## Identify the extreme PC1 values

```{r}
sort(decreasing=T,(pca.diamonds$x[,1]))[1:10]
```

## PCA with Smaller Sample

```{r, echo=FALSE}
set.seed(300525287, kind = "Mersenne-Twister")
smallersample <- diamonds[sample(nrow(diamonds), "1000"), ]
reducedpca <- prcomp(smallersample[,-c(2:4,11)], center = TRUE, scale = TRUE)
```

```{r, echo=FALSE}
pca.var_red <- reducedpca$sdev^2
pca.var.per_red <- round(pca.var_red/sum(pca.var_red)*100,1)

```

```{r pcascreered, fig.cap="Scree plot of the Principal Components (reduced dataset)", echo=FALSE}
barplot(pca.var.per_red, names.arg = pca.var.per_red,
        xlab = "Principal Components 1 to 7",
        ylab = "% information retained by each PC",
        main="Screeplot of the Principal Components (reduced dataset)")

```

Figure \@ref(fig:pcascreered) shows the scree plot of the Principal Components for the reduced dataset. PC1 contains `r pca.var.per_red[1]`% of the variance, PC2 contains `r pca.var.per_red[2]`%, while PC3 contains `r pca.var.per_red[3]`%. Between them they contain `r sum(pca.var.per_red[1:3])`%.


```{r}
summary(reducedpca)
```

We see from the summary output above that the PCA with a sample size of 1000 has almost identical properties to the PCA of the whole dataset, which allows us to proceed.

```{r biplotpcared, fig.cap="Biplot of the reduced dataset PC1 vs PC2", echo=FALSE}
ggbiplot(reducedpca, obs.scale =1, var.scale = 1, varname.size = 5)
```

Producing a biplot using the reduced dataset allows us to interpret the biplot much better (figure \@ref(fig:biplotpcared)). We see that carat, x, y, z and price are strongly influence the first PC. While depth and table strongly (and reasonably equally) influence the second PC. We see evidence of the negative correlation between table and depth and a close to zero correlation between these two variables and the variables that strongly influence PC1. We also have an indication that there is redundancy between carat, x, y, z and price.  The percentage of variation retained by each Principal Component is included in the axis labels.

In the above analysis we have shown we are able to us principal components analysis to reduce the number of dimensions in the dataset while still retaining a large proportion of its variation. In this investigation we are interested in predicting the price of diamonds. To do this we will perform a regression with principal components with price as the dependent variable and the other principal components as explanatory variables. 

## Principal Components Regression

We will now perform a principal regression analysis with price as the response variable and and all other numerical variables as the explanatory variables. We Will again use the smaller sample to increase the ease of interpretation of the visual plots. Below we see an initial multiple regression using only the numeric variables

```{r}
numericlm <- lm(scale(price) ~ scale(carat) + scale(table) + scale(depth) + scale(x) + scale(y) + scale(z), data = diamonds_num)
summary(numericlm)
```

In this model all but one (z) of the seven numeric predictors are found to be significant in the model. This model explains 86% of variation in price. We will now attempt to create a more parsimonious model using the principal components. 

```{r, echo=FALSE}
PCAprice <- prcomp(smallersample[,-c(2:4,7,11)], center = TRUE, scale = TRUE)
```

```{r, echo=FALSE}
pca.varpri <- PCAprice$sdev^2
pca.varpri.per <- round(pca.varpri/sum(pca.varpri)*100,1)

```

```{r pcascreereg, fig.cap="Scree plot of the regression Principal Components", echo=FALSE}
barplot(pca.varpri.per, names.arg = pca.varpri.per,
        xlab = "Principal Components 1 to 7",
        ylab = "% information retained by each PC",
        main="Screeplot of the regression Principal Components")

```

Figure \@ref(fig:pcascreereg) shows the scree plot of the proportion of variance explained by each of the Principal Components of the PC regression. The first three principal components explain nearly all (99%) the variation in the data.

```{r}
PCAprice$rotation
```

We see that in the first PC carat, x, y and z all have approximately equal weighting. Again this reflects the strong positive correlation between these two variables. In the second PC depth and table have the strongest weighting. We see that removing price from the PCA does not change much in terms of how much variation the first few components explain as well as which variables contribute most strongly to each component. This is because of how strongly price is correlated with the dimension variables. 

```{r, echo=FALSE}
ggbiplot(PCAprice, obs.scale =1, var.scale = 1, varname.size = 5)
```

Even with price not included we still see the same general structure as the PCA with price.

```{r}
PCApricevars <- prcomp(smallersample[,-c(2:4,7,11)],
                       center = TRUE, scale = TRUE)$x
diamondpricePCA <- lm(smallersample$price ~ PCApricevars[,1] + PCApricevars[,2]
                      + PCApricevars[,3] + PCApricevars[,4] + PCApricevars[,5] +
                        PCApricevars[,6])
summary(diamondpricePCA)
```

We see that every single principal component is found to be significant in this model when the effects of the other principal components is taken into account. Therefore initially it seems we have failed to create a more parsimonious model for predicting price through using regression with principal components. However if we refit the model with just the first two principal components (see below) we still create a model that is able to explain over 81% of the variance in price with only two components. While this is not as good as the model with all the principal components (which explains 85%) it is still reasonably good and is much more simple. The original numeric multiple regression model explained 86% of the variation in price with 6 explanatory variables while our PCR model explains 81% with just the first two principal components. Principal components regression has allowed us to explain nearly the same amount of variation in price in a much more parsimonius way.  

```{r}
diamondpricePCA2 <- lm(smallersample$price ~ PCApricevars[,1] + PCApricevars[,2])
summary(diamondpricePCA2)
```


It is likely that the remaining, unexplained variation could be explained by the categorical variables in the dataset. As Principal Components Analysis/Regression can only be used for numerical variables we had to leave out the variables of cut, clarity and color.


# Factor Analysis

In our investigation of the principal components of the dataset we observed that the first two principal components explained the the vast majority of the variation in the data.  The first was dominated by the strong positive correlation between price and the dimension variables (x, y, z, carat) which we may call price + dimension. The second was the negative correlation between depth and table which we might combine into light performance as both variables are crucial in giving a diamond its "sparkle". We hypothesise that these two factors (1. "Price + Dimension" 2. "Light Performance") may explain the unobserved variable of "overall diamond quality". To investigate this we will conduct an factor analysis with two factors.  

```{r}
factanal(diamonds[,-c(2:4,11)], factors =2)
```

As we hypothesized, 'carat', 'price', 'x', 'y' and 'z' are well explained by factor 1. 'Depth' is very well explained by factor 2, but 'table' is explained only to a lesser degree. We see some evidence of the factor structure in our hypothesis with the 'price + dimension' variables strongly explained in Factor 1 and 'Light Performance' variables most strongly explained by factor 2. However, we also see other original variables we did not expect in each of our factors. If our hypothesis was correct we would see a negligible loading of depth and table in factor 1 and a negligible loading for the price + dimension variables in factor 2.

Factor 1 explains 66% of the variance in the data and Factor 2 explains an additional 16%. Together are two factors manage to explain 83% of the variation in the data. We also see high uniqueness ratings for 'table' indicating that the two factors do not well account for it's variance. 

Overall the hypothesis test gives a highly significant result meaning that two factors are not sufficient to capture the full dimensionality of the data set.

# Linear Discriminant Analysis (LDA)

As discussed previously the diamonds data set has three categorical variables each with many levels. This means we have the possibility of 280 unique interactions between the levels of these categorical variables.This many factors is unlikely to be that useful as a classification tool. The secondary aim of our project was to investiage if we could classify the diamonds dataset more simply. We will begin by attempting a Linear Discriminant Analysis using the factors.

```{r, warning= FALSE, echo=FALSE}
diamonds$Interaction <- interaction(diamonds$cut, diamonds$color, diamonds$clarity)
set.seed(123456789, kind = "Mersenne-Twister")
ind <- sample(c("Train", "Test"),
              nrow(diamonds),
              replace = TRUE, 
              prob = c(.8, .2))
diamonds.Train <- diamonds[ind == "Train",]
diamonds.Test <- diamonds[ind == "Test",]
LDAinteraction <- lda(Interaction ~ carat + depth + table + price + x + y + z, data = diamonds.Train)
```

We will not include the output from the LDA using the interactions as it is very long given there are 280 classes. Instead we will report the key finding; the overall accuracy of using the interactions as a classifier. 

```{r, echo= FALSE}
RPinteraction <- predict(LDAinteraction, diamonds.Test)$class
RCMinteraction <- table(RPinteraction, actual = diamonds.Test$Interaction)
sum(diag(RCMinteraction))/sum(RCMinteraction)
```

An overall accuracy of 0.065 is very low and therefore we can conclude that using the factors as classes was not effective. It's likely the data is not linearly separable into 280 classes. We will now instead attempt to use Linear Discriminant Analysis to classify observations more simply using levels of each of the categorical variables. We will review how each of these categorical variables perform as classifiers.


```{r}
LDAcut <- lda(cut ~ carat + depth + table + price
              + x + y + z, data = diamonds.Train)
LDAcolor <- lda(color ~ carat + depth + table + price
                + x + y + z, data = diamonds.Train)
LDAclarity <- lda(clarity ~ carat + depth + table + price
                  + x + y + z, data = diamonds.Train)
```
  


```{r ordplot, fig.cap="Ordination plots", echo=FALSE}

g1 <- ggord(LDAcut, diamonds.Train$cut, alpha = 0.5, )
g2 <- ggord(LDAcolor, diamonds.Train$color, alpha = 0.5 )
g3 <- ggord(LDAclarity, diamonds.Train$clarity, alpha = 0.5)
ggarrange(g1, g2, g3, ncol = 2, nrow =2)
```

All three ordination plots (figure \@ref(fig:ordplot)) show a lot of overlap between different levels of the categorical variables. It seems unlikely that a linear discriminant function would have much success in partitioning the data according to these classes. We shall compute the confusion matrix for each LDA and assess the overall accuracy of each model.

```{r, echo=FALSE}
RPcut <- predict(LDAcut, diamonds.Test)$class
RCMcut <- table(RPcut, actual = diamonds.Test$cut)
RPcolor <- predict(LDAcolor, diamonds.Test)$class
RCMcolor <- table(RPcut, actual = diamonds.Test$color)
RPclarity <- predict(LDAclarity, diamonds.Test)$class
RCMclarity <- table(RPcut, actual = diamonds.Test$clarity)
```

```{r, echo=FALSE}
Cutacc <- sum(diag(RCMcut))/sum(RCMcut)
Coloracc <- sum(diag(RCMcolor))/sum(RCMcolor)
Clarityacc <- sum(diag(RCMclarity))/sum(RCMclarity)
```

```{r LDAacc, echo = FALSE}
LDAacc <- data.frame(Class=c("Cut","Color","Clarity"),
                      Accuracy=c(0.6258, 0.1738, 0.2039))

knitr::kable(LDAacc,caption = "Classification Accuracy for Different LDAs")
```

Table \@ref(tab:LDAacc) displays the accuracy of the classification achieved by the LDA for the different categorical variables. The only variable that was classified with any success was 'cut'. For 'color' and 'clarity' the LDA performs poorly. As "cut" was the only class to perform moderately well as classifier we will investigate its performance further. 

```{r}
LDAcut
```

The first thing we notice is that the prior probabilities for each class are very different. We see small amounts of variation in group means for different variables across different classes but there is nothing to striking. As the data are on different scales it is harder to interpret the group mean differences. The variables with strongest weight in LD1 are carat, x, y and table while the variables with the most weight in LD2 are x, y, carat and depth. LD1 retains almost 80% of the variation in the data while LD2 retains only 17%.

```{r}
confusionMatrix(RCMcut)
```

The confusion matrix clearly shows a large amount of errors for each individual class. We again get the overall accuracy measurement of 0.6258 with a 95% confidence interval of (0.6166,0.6349). In terms of the accuracy of classification for individual classes Ideal, Fair and Premium had higher accuracy ratings than Good or Very Good. It was important to check individual class accuracy as the prior probabilities of an observation belonging to each class were very different. 

We conclude that using the levels of the categorical variables as classes was more successful than using the interactions with the cut variable returning the highest accuracy. Even this was not particularly succesful. We therefore conclude the data is not linearly separable by using the levels of categorical variables as classes. 


## Why prediction without the categorical variables is not working well

### Create modified version of dataset only containing carat values between 1 and 1.1

```{r, warning=FALSE, message=FALSE}
library(dplyr)
diamonds_car2 <- diamonds[,c(1,4,7)]
diamonds_car2[1:4,]
diamonds_car19_21 <- filter(diamonds_car2, between(carat, 1, 1.1))
diamonds_car19_21[1:4,]
```




```{r carat1to11clarity, fig.cap="Carat (limited values) vs Price coloured by Clarity", echo=FALSE}

# scatterplot of carat vs price, but also colouring the observations
# by 'cut'
ggplot(data=diamonds_car19_21, aes(x=carat, y=price, color=clarity))+
  geom_point()+
  guides(colour=guide_legend(reverse = T))+
  labs(title = "'Carat' vs 'price', coloured by 'clarity'")
```

Figure \@ref(fig:carat1to11clarity) shows a the scatterplot of 'price' vs 'carat' vs 'clarity', but just for values of 'carat' between 1 and 1.1, which helps us see the pattern more clearly. Firstly, note the vertical spread of the data points; most of the values begin at around 2500 USD and generally have extreme upper values larger than 15,000 USD, so a spread of approximately 10,000 USD. There are clear colour bands running horizontally, with the lowest quality 'clarity' diamonds near the bottom of the price range, while the highest values are near the top.

So, using 'carat' alone to predict these prices clearly is not enough; given the spread of price for any given value of carat, the categorical variables play an important in determining where in the range any given diamond will be. This was the reason our regression models that excluded the categorical variables did not perform nearly as well as those that included them.

## Discriminant analysis using the reduced dataset (price, carat and clarity)

To address the issue above, we attempted another discriminant analysis using the reduced dataset above, that is, for 'carat' values between 1.0 and 1.1. The hope was that the relatively clear bands seen in figure \@ref(fig:carat1to11clarity) would enable the LDA to this time, as there is less overlap between levels in the reduced dataset than there is in the original.

So, we repeat the Linear Discriminant Analysis process using the reduced dataset.

```{r,}
# split dataset into 'train' and 'test'
set.seed(1234567890, kind = "Mersenne-Twister")
ind <- sample(c("Tr","Te"),
              nrow(diamonds_car19_21),replace = TRUE,prob = c(0.6,0.4))

# display first few elements to check
ind[1:15]
```

```{r}
# check that the proprortions have come out as expected
nrow(diamonds_car19_21)
table(ind)
prop.table(table(ind))
```

The output above confirms that the random allocation of values to either the training or testing datasets produced the desired result of approximately 60% in the training set and 40% in the testing set. 

### Create training and test subsets

```{r}
Train <- diamonds_car19_21[ind=="Tr",]
Test <- diamonds_car19_21[ind=="Te",]

```

### Create LDA and display summary

```{r}
library(MASS)
LDA.diam <- lda(clarity~carat+price,
           data=Train)

```

### Boxplots of the LDA means

```{r ldameans, fig.cap="Distribution of LDA means", echo=FALSE}
boxplot(t(LDA.diam$means), main=
          "Distribution of LDA means for each level of 'clarity'",
        xlab="Levels of 'clarity' (lowest on the left, highest on the right)",
        ylab="Count")
```

Figure \@ref(fig:ldameans) shows the distribution of the LDA means. It is obvious that there is an increasing number of diamonds of higher 'clarity' levels.

### Create Prediction object

```{r}
Pred <- predict(LDA.diam)

```

### Biplot

```{r biplot2, fig.cap="Biplot of 'carat', 'price' against 'clarity'", echo=FALSE}
library(klaR)
library(ggord)
library(devtools)
ggord(LDA.diam,Train$clarity)
```

Figure \@ref(fig:biplot2) shows the biplot for 'carat' versus 'price' against the different levels of 'clarity'.  There is still a large amount of overlap visible near the top of the plot (the different coloured dots), meaning that separation of the levels is not going to be easy.

### Partition plot

```{r partplot2, fig.cap="Partition plot for levels of 'clairty' as predicted by 'carat' and 'price'", echo = FALSE}
partimat(clarity~carat+price, data = Train, method="lda")
```

Figure \@ref(fig:partplot2) shows the partition plot for the discriminant analysis (DA) of the different levels of the 'clarity' variable as predicted by 'carat' and 'price'. As a reminder, the levels are: I1 (lowest clarity), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (highest clarity).  We can see from the number of red font values that there are many mis-categorised observations, suggesting that the LDA has not worked particularly well.


```{r, echo=FALSE}
RealisticPredicted <- predict(LDA.diam,Test)$class
RCM <- table(RealisticPredicted, Actual=Test$clarity)
RCM
```

The confusion matrix above shows that there were many mis-categorisations. There is a pattern evident. The leading diagonal shows the correct predictions. There are many incorrect predictions for values immediately above and below the leading diagonal, meaning that the DA partitions struggled to separate adjacent categories. 

```{r}
prop_succ <- sum(diag(RCM))/sum(RCM)
prop_succ

per_succ <- signif(prop_succ*100,4)
per_succ
```

The output above shows that the percentage of successful predictions was `r per_succ`%, indicating that this LDA is a mediocre predictor. 

### Summary of LDA

The relatively poor performance of LDA in this instance is perhaps not surprising given the amount of overlap we saw in figure \@ref(fig:carat1to11clarity) earlier (the scatterplot of 'price' vs 'carat' vs 'clarity' for 'carat' values between 1 and 1.1); there simply isn't enough geometrical separation between the different levels of 'clarity' for the algorithm to accurately categorise the boundary cases.

# Cluster Analysis

As LDA did not yield useful results in terms of classifying the diamonds data we also decided to conduct Cluster Analysis to investigate if the diamonds data set could be divided into clusters. We used a K-means algorithm to achieve this. As we were initially unsure of how many clusters we should attempt to divide the data into we will first examine the sum of the squares of the intra-class disatances for different numbers of clusters. We will again use the smaller sample to aid in the interpretation of visual displays. 

## Optimal cluster number

```{r optclust, fig.cap="Optimal number of clusters", echo=FALSE}
fviz_nbclust((scale(smallersample[,-c((2:4),11)])), kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```

Figure \@ref(fig:optclust) shows that when the elbow method is used, we see that 4 clusters is the optimum number.It is worth noting that the total within sum of squares is large for any selected number of clusters. 

## kmeans function

```{r,}
clusters <- kmeans(scale(smallersample[,-c((2:4),11)]), 
                   algorithm = "MacQueen", centers = 4, 
                   iter.max = 100, nstart = 50)


clusters$centers

```

We create four clusters with sizes of 127, 455, 137 and 281 respectively. We see clear differences in the mean of each numeric variable across the four clusters. We see that cluster 3 contains diamonds with a higher mean price, carat, x, y and z. While cluster 2 contains smaller, less valuable diamonds. Diamonds in cluster 4 have larger absolute mean scores for depth and table compared to the other clusters. 


```{r}
clusters$withinss
```

The within cluster sum of squares indicates that the clusters are not very compact. We shall now visualize the clusters with a scatterplot using the first two principal components of our dataset as the two dimensions. The first two principal components account for over 86% of the variance so this should be an appropriate visualization.

## Cluster plot

```{r clustplot, fig.cap="Cluster plot", echo = FALSE}
fviz_cluster(clusters, data = (scale(smallersample[,-c((2:4),11)])),
             geom = "point",
             ellipse.type = "norm", 
             ggtheme = theme_bw()
             )
```

In the cluster plot (figure \@ref(fig:clustplot)) we see significant overlap between the four clusters as well as many observations that do not seem to fit well in any of the clusters. It seems that the diamond dataset does not easily split into distinct clusters. 

It is worth mentioning that 'diamonds' already has categories and levels within those categories, and that the clustering above does not take the pre-existing categories into account, but attempts to find naturally occurring classes within the data. 

## Silhouette plot

```{r silh, fig.cap="Clusters silhouette plot", echo = FALSE}
library(cluster)
sil <- silhouette(clusters$cluster, dist((scale(smallersample[,-c((2:4),11)]))))
fviz_silhouette(sil)
```

The silhouette plot (figure \@ref(fig:silh)) helps us to visualize how similar points each observation is to the cluster that it is assigned too. We see very few negative values indicating that very few observations have been assigned to the wrong cluster. However we do have a large proportion of values with low positive values indicating that these observations were close to the borderline between two clusters. We see that the average silhouette width is 0.38. 

# Conclusion

During this investigation we applied a number of the new analytical methods that we learned in STAT394. As this project was principally a learning task we attempted them all, even though an experienced statistician may have concluded beforehand that they were inappropriate or of dubious value. We mostly performed these techniques on a subset of the data that only included the numerical variables. Not all of the analytical techniques worked as we had hoped, and when this occurred we investigated why.

The primary aims of our  multivariate statistical investigation on the diamonds data set were as follows  

- To find the best way of predict diamond price using the other variables in the data set  

- To attempt to classify observations, in particular the different levels of the three categorical variables   

- To explore the new statistical techniques of PCA, LDA, FA and CA in relation to our data set  


We had the most success with our first aim; predicting diamond price. This was first achieved using the multiple regression model with all variables except for 'y' and 'z', in other words the model; price = carat + cut + color + clarity + depth + table + x. The best model was obtained using stepwise AIC and BIC tests. The diagnostics for the multiple regression model showed that all the assumptions were violated, meaning the model might not produce accurate predictions.  Secondly, a principal components analysis using the numerical variables was successful in reducing the dimensionality of the dataset while still retaining a large proportion of the variance. We were able to use the first two principal components in a model using principal components regression and this performed almost as well as the "best" model identified through stepwise multiple regression despite being much more parsimonious. We did however note that a model including all principal components was more effective at predicting price. 

An examination of the key principal components inspired us to conduct a factor analysis with the hypothesis that the factors "price + dimension" and "light conductance" might be sufficient to explain variation in the dataset. While we saw evidence of our hypothesized structure in the factor loadings, two factors was not found to be sufficient to capture all the variation in the data. 

We attempted to address our second aim through the techniques of LDA and CA. A LDA was undertaken using each categorical variable as a classifier. This failed to reliably discriminate observations. As using known classes in the dataset was unsuccessful we attempted a cluster analysis. The cluster analysis identified four clusters as the optimal number to describe the data however there was significant overlap between these.  We conclude that the diamonds dataset presents a difficult classification problem and that the reason for this is the high number of possible interactions between levels of our three categorical variables.  


# Appendices

## Code for Producing KS Tests

```{r}
ks.test(diamonds$carat, "pnorm", mean=mean(diamonds$carat), sd=sd(diamonds$carat))
ks.test(diamonds$depth, "pnorm", mean=mean(diamonds$depth), sd=sd(diamonds$depth))
ks.test(diamonds$table, "pnorm", mean=mean(diamonds$table), sd=sd(diamonds$table))
ks.test(diamonds$price, "pnorm", mean=mean(diamonds$price), sd=sd(diamonds$price))
ks.test(diamonds$x, "pnorm", mean=mean(diamonds$x), sd=sd(diamonds$x))
ks.test(diamonds$y, "pnorm", mean=mean(diamonds$y), sd=sd(diamonds$y))
ks.test(diamonds$z, "pnorm", mean=mean(diamonds$z), sd=sd(diamonds$z))
```


## Code for producing ANOVA/Levene's Test/Kruskal Wallis/Tukey of Price by Cut

```{r}
cutanova <- aov(price ~ cut, data = diamonds)
summary(cutanova)
leveneTest(price ~ cut, data= diamonds)
kruskal.test(price ~ cut, data = diamonds)
TukeyHSD(cutanova, conf.level = 0.95)
```

## Code for producing ANOVA/Levene's Test/Kruskal Wallis/Tukey of Price by Clarity

```{r}
clarityanova <- aov(price ~ clarity, data = diamonds)
summary(clarityanova)
leveneTest(price ~ clarity, data= diamonds)
kruskal.test(price ~ clarity, data = diamonds)
TukeyHSD(clarityanova, conf.level = 0.95)
```

##  Code for producing ANOVA/Levene's Test/Kruskal Wallis/Tukey of Price by Color

```{r}
coloranova <- aov(price ~ color, data = diamonds)
summary(coloranova)
leveneTest(price ~ color, data= diamonds)
kruskal.test(price ~ color, data = diamonds)
TukeyHSD(coloranova, conf.level = 0.95)
```

# Bibiography
